## Sparkify - Data Warehouse

A music streaming startup, Sparkify, has grown their user base and song database and want to move their processes and data onto the cloud. Their data resides in S3, in a directory of JSON logs on user activity on the app, as well as a directory with JSON metadata on the songs in their app.

The goal of the project is building an ETL pipeline that extracts their data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for the analytics team to continue finding insights into what songs their users are listening to. 

## Database schema:

There are two major datasets that reside in S3:

1- Song dataset: s3://udacity-dend/song_data
Each file is in JSON format and contains metadata about songs and the artists of the songs.

2- Log dataset: s3://udacity-dend/log_data
These consists log files in JSON format generated by event simulator based on the songs in the dataset above which simulate activity logs from a music streaming app based on specified configurations.

At first we copy data from the files in S3 to two staging tables on Redshift:

1- staging_events: contains 
artist, auth, firstName, gender, itemInSession, lastName, length, level, location, method, page, registration, sessionId, song, status, ts, userAgent, userId.

2- staging_songs: contains
num_songs, artist_id, artist_latitude, artist_longitude, artist_location, artist_name, song_id, title, duration, year.

Then a star schema optimized for queries. This includes the following tables.

 1- Fact table called (songplays) 
 records in log data associated with song plays and records "songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent"


 2- Four dimension tables called (users, songs, artists and time.)
 2.1- users: users in the app and contains "user_id, first_name, last_name, gender, level"
 2.2 - songs: songs in music database and contains "song_id, title, artist_id, year, duration"
 2.3- artists: artists in music database and contains "artist_id, name, location, latitude, longitude"
 2.4- time:time - timestamps of records in songplays broken down into specific unitsand contains "start_time, hour, day, week, month, year, weekday"
 
## Project Structure

create_table.py : is where fact and dimension tables created for the star schema in Redshift.
etl.py : where the data from S3 loaded into staging tables on Redshift and then processed into the analytics tables on Redshift.
sql_queries.py : where SQL statements definied, which will be imported into the two other files above.
README.md : discussion of the process.

# Running Scripts:

1- Launch a redshift cluster and create an IAM role that has read access to S3.
2- Add redshift database and IAM role info to dwh.cfg.
3- Run create_tables.py and check the table schemas in the redshift database. You can use Query Editor in the AWS Redshift console for this.
4- run etl.py 
5- Delete your redshift cluster when finished.




